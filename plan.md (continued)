### Objectives
- Wire the React Timeline editor to a backend that can queue video generation, enforce VRAM preflights, and stream progress from the existing telemetry pipeline.
- Harden the defaults (low VRAM, quantized WAN 2.2 models, CFG/step caps) so the newly wired Generate button does not regresses on 20–24 GB rigs.
### Tasks
1. Create `POST /api/generate-video` (or similar) in the Node/Express backend that enqueues the job through `services/generationQueue.ts`, triggers `scripts/generate-scene-videos-wan2.ps1`, and updates the Artifact Snapshot/Timeline UI via the queue’s progress callbacks.
2. Extend the React Timeline editor to call this endpoint, render per-scene progress from `artifact-metadata.json`, display queue warnings, and only enable Generate when the queue/vram gate passes (`services/generationQueue.ts` + `/system_stats`).
3. Update `localGenSettings.json`, the WAN workflows under `workflows/`, and the health helper scripts so `--lowvram` and GGUF WAN 2.2 models load by default. Expose `LOCAL_COMFY_LOWVRAM`/`WAN2_MODEL_FORMAT` toggles for advanced users.
4. Apply safe defaults for CFG (~5.2) and diffusion steps (12–20) in `localGenSettings.json` and the WAN workflows, keep the prompt injector logging seed/CFG (current script already writes these), and add warning logic in the backend when CFG > 6 so the UI can surface the “black frames” concern mentioned in the README.
5. Offer optional LoRA & ControlNet templates (new workflow JSON in `workflows/` referencing `models/loras/*`) and UI fields so the generator can automatically inject LoRA weights and ControlNet anchors when provided.
6. Strengthen VRAM preflight gating in `services/generationQueue.ts` (min VRAM check, queued job diagnostics, circuit breaker) and expose its status via the new API so the UI can disable Generate when insufficient VRAM is reported.
### Testing & Documentation
- Write Vitest/Playwright tests that simulate the Generate button flow, ensure the queue rejects new jobs when VRAM is low, and assert telemetry lines (duration, GPU/VRAM) remain present (`README.md:872`). Update `README_TESTING_STARTS_HERE.md` and `README.md` to mention the new endpoint, queue wiring, and preset defaults.
- Document how to download GGUF WAN models, supply LoRA/ControlNet assets, and interpret queue telemetry in a new guide under `docs/` or `Documentation/`.

## 3. Phase 2 – FLF2V + Post-Processing

### Objectives
- Improve narrative continuity by chaining final frames and enhance video quality with interpolation/upscaling while keeping instrumentation in sync with the helper scripts.
### Tasks
1. Extend the orchestrator or PowerShell script so the last frame of scene N is stored (`logs/<ts>/<sceneId>/frame-last.png`) and fed into scene N+1’s WAN I2V `init_image`. Add CLI flags like `--flf2v` and `--last-frame-model`, reuse the “producer done marker” knitting already present in the scripts, and verify continuity with `scripts/bookend-frame-similarity.ts`.
2. Integrate a RIFE interpolation step (Python wrapper or ComfyUI node) that reads each scene’s MP4 and writes an interpolated version at 24 fps. The pipeline must gracefully skip interpolation when CPU/GPU resources are constrained (warn via telemetry).
3. Attach Real-ESRGAN upscaling (script or ComfyUI node) after interpolation, expose resolution/scale options to the UI, and ensure audio is either preserved or documented as stripped.
4. Extend `artifact-metadata.json` and the Artifact Snapshot UI to include interpolation/upscale metadata (`InterpolationElapsed`, `UpscaleMethod`, final resolution), preserving the telemetry contract described in `README.md:872`.
### Testing & Documentation
- Validate final frame chaining with the bookend similarity script, assert interpolated video frame rates with `ffprobe`, and confirm upscaled resolution via automated checks.
- Update docs (e.g., `docs/STORY_TO_VIDEO_PIPELINE_PHASE_*.md`) with the FLF2V toggle, recommended interpolation/upscale combos per preset, and cost/VRAM notes for RIFE/Real-ESRGAN.

## 4. Phase 3 – Infinite Video & Advanced Control

### Objectives
- Allow long-form generation (SVI) and precise character control while keeping these features opt-in.
### Tasks
1. Add ComfyUI SVI workflows (`workflows/video_wan2_svi.json`) plus LoRA files under `models/loras/svi/`, expose an “SVI Mode” toggle in the UI, and ensure the orchestrator chooses the correct workflow and telemetry tags when enabled. Warn users about the >1 GB LoRA footprint.
2. Accept zipped pose/depth sequences in the Timeline UI and pipe them through a new `services/controlNetUploader.ts` that validates counts and posts `ControlNet` inputs per frame, optionally leveraging ComfyUI’s AnimationControlNet node.
3. Add alternative video providers (FastVideo + SVD) to the provider dropdown, reuse the FastVideo scripts (`scripts/run-fastvideo-server.ps1`) and tests (`tests/integration/fastvideo.test.ts`), and document the pros/cons in `README.md:309` (FastVideo section) and the new plan.
### Testing & Documentation
- Provide regression tests comparing ControlNet vs baseline scenes for pose adherence. For SVI, reuse the frame similarity infrastructure to confirm identity stability across at least three scenes.
- Update `README.md`’s FastVideo and SVD sections with new UI controls and highlight when the queue should fallback to ComfyUI.

## 5. Phase 4 – Keyframes & Seamless Transitions

### Objectives
- Generate premium keyframes via Z-Image Turbo and blend scenes using VACE transitions.
### Tasks
1. Create a Z-Image Turbo keyframe generator (`scripts/generate-keyframe-zimage.ts`) that can fallback to WAN T2I when Z-Image is unavailable; update each scene to call this script before WAN I2V and store results in `scene/keyframe.png`.
2. Add a WAN VACE Clip Joiner workflow (`workflows/vace_joiner.json`) plus `scripts/join-clips-vace.ps1` (or Python) to glue scenes together. Allow the final pipeline to output one joined MP4, optionally run before interpolation/upscaling, and expose VACE knobs (transition frames, context frames) in the UI.
3. Make the timeline reorderable (drag/drop) and ensure story metadata plus FLF2V chaining respect the new order. Update `story.json` whenever the user rearranges scenes.
### Testing & Documentation
- Validate Z-Image keyframes against WAN outputs via QA cards, ensure VACE transitions do not introduce color drift (add histogram matching fallback if needed), and test that reordering scenes updates `logs/<ts>/scene-order.json`.
- Document Z-Image/VACE prerequisites and safeguards in `README.md` and `Documentation/` guides.

## 6. Phase 5 – UI, Persistence & Orchestration Hygiene

### Objectives
- Surface timeline/storyboard controls, orchestrator health, project import/export, and LLM-assisted prompting directly in the React app.
### Tasks
1. Build the horizontal timeline component with thumbnails, duration markers, generation status badges, drag/drop reordering, and artifact preview links, persisting projects in IndexedDB/local files (mirroring Gausian’s persistence ideas).
2. Show ComfyUI/queue health (VRAM, running tasks, warnings) via existing health helper outputs (`scripts/comfyui-status.ts`), let users toggle the orchestrator mode (PowerShell vs Python), and keep status badges in sync with queue telemetry.
3. Implement Export/Import buttons that emit/consume schema-compatible JSON/EDL (align with `docs/STORY_TO_VIDEO_PIPELINE_PHASE_1.md`), and ensure Vitest/Playwright cover the round-trip.
4. Add an interactive story assistant panel (chat UI) that reuses `services/localStoryService.ts` prompts and records suggestions in project metadata.
### Testing & Documentation
- Ensure timeline UI tests cover drag/drop, persistence, preview rendering, and fallback messaging when generation fails. Document the project import/export schema and story assistant workflow in `Documentation/` guides.

## 7. Phase 6 – Python Orchestrator & Performance Monitoring

### Objectives
- Replace the heavyweight PowerShell pipeline with a maintainable Python orchestrator while keeping multi-GPU/performance monitoring intact.
### Tasks
1. Write `scripts/orchestrator.py` that mirrors `scripts/generate-scene-videos-wan2.ps1` (keyframes → WAN I2V → post-processing → VACE), talks to ComfyUI (`requests`/`websocket`), writes `run-summary.txt`/`artifact-metadata.json`, and exposes CLI flags for SVI/FLF2V/VACE/interpolation/upscaling.
2. Allow GPU assignments per stage via env vars, log VRAM usage (via `nvidia-smi` or `py3nvml`), and fallback to CPU when needed. Document device mappings and recommended headroom (8–24 GB) from `README.md`.
3. Record timings/memory usage per stage with `psutil`, store them in telemetry, and surface handful metrics in the UI (e.g., stage duration, VRAM delta).
4. Extend CI (Vitest + Playwright) and pytest suites to run the orchestrator on a short project, validating telemetry, FLF2V continuity, and post-processing success with existing QA tooling (`README.md:117-155` testing instructions).
### Testing & Documentation
- Update `README.md`, `README_TESTING_STARTS_HERE.md`, and `task` docs to mention the Python orchestrator, its CLI flags, and how to fall back to the PowerShell scripts. Add pytest coverage for each orchestrator stage.

## 8. Cross-Phase Governance

- **Documentation:** Every new workflow or CLI flag must have an accompanying document under `docs/` or `Documentation/` and an entry in `DOCUMENTATION_INVENTORY.md`.
- **Telemetry/QA:** Use the helper scripts (`scripts/validate-run-summary.ps1`, `scripts/bookend-frame-similarity.ts`) to gate releases. When telemetery fields diverge, block merges until logs are reconciled.
- **User Experience:** Advanced features remain behind presets (`Fast`, `Standard`, `Cinematic`). The UI should warn when a new feature exceeds VRAM or fails and suggest fallback recipes.
- **Next Steps:** After each phase, gather telemetry/QA reports (e.g., `test-results/...`) and refine presets before unlocking the subsequent phase for a broader audience.
